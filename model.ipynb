{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noemi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\noemi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 31083/31083 [00:02<00:00, 13733.89 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3886/3886 [00:00<00:00, 11337.66 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3885/3885 [00:00<00:00, 14974.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load your dataset (replace with actual loading code)\n",
    "data = pd.read_csv('data/train_submission.csv')\n",
    "\n",
    "# Map labels to numerical IDs\n",
    "labels = data['Label'].unique()\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "data['LabelID'] = data['Label'].map(label2id)\n",
    "\n",
    "# Split the dataset into train/test/validation (80/10/10 split)\n",
    "train_data = data.sample(frac=0.8, random_state=42)\n",
    "remaining_data = data.drop(train_data.index)\n",
    "val_data = remaining_data.sample(frac=0.5, random_state=42)\n",
    "test_data = remaining_data.drop(val_data.index)\n",
    "\n",
    "# Convert to Hugging Face Dataset format\n",
    "def convert_to_dataset(df):\n",
    "    return Dataset.from_pandas(df[['Text', 'LabelID']])\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': convert_to_dataset(train_data),\n",
    "    'validation': convert_to_dataset(val_data),\n",
    "    'test': convert_to_dataset(test_data)\n",
    "})\n",
    "\n",
    "# Load a multilingual model (e.g., XLM-Roberta)\n",
    "# model_name = \"xlm-roberta-base\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))\n",
    "\n",
    "\n",
    "# Load a multilingual model (e.g., DistilBERT Multilingual)\n",
    "model_name = \"distilbert-base-multilingual-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))\n",
    "\n",
    "# Tokenize the dataset\n",
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['Text'], truncation=True, padding=True, max_length=100)\n",
    "    inputs[\"labels\"] = examples[\"LabelID\"]  # Ensure labels are included for loss calculation\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noemi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\noemi\\AppData\\Local\\Temp\\ipykernel_28572\\4073686792.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "Trainer is attempting to log a value of \"{0: 'LABEL_0', 1: 'LABEL_1', 2: 'LABEL_2', 3: 'LABEL_3', 4: 'LABEL_4', 5: 'LABEL_5', 6: 'LABEL_6', 7: 'LABEL_7', 8: 'LABEL_8', 9: 'LABEL_9', 10: 'LABEL_10', 11: 'LABEL_11', 12: 'LABEL_12', 13: 'LABEL_13', 14: 'LABEL_14', 15: 'LABEL_15', 16: 'LABEL_16', 17: 'LABEL_17', 18: 'LABEL_18', 19: 'LABEL_19', 20: 'LABEL_20', 21: 'LABEL_21', 22: 'LABEL_22', 23: 'LABEL_23', 24: 'LABEL_24', 25: 'LABEL_25', 26: 'LABEL_26', 27: 'LABEL_27', 28: 'LABEL_28', 29: 'LABEL_29', 30: 'LABEL_30', 31: 'LABEL_31', 32: 'LABEL_32', 33: 'LABEL_33', 34: 'LABEL_34', 35: 'LABEL_35', 36: 'LABEL_36', 37: 'LABEL_37', 38: 'LABEL_38', 39: 'LABEL_39', 40: 'LABEL_40', 41: 'LABEL_41', 42: 'LABEL_42', 43: 'LABEL_43', 44: 'LABEL_44', 45: 'LABEL_45', 46: 'LABEL_46', 47: 'LABEL_47', 48: 'LABEL_48', 49: 'LABEL_49', 50: 'LABEL_50', 51: 'LABEL_51', 52: 'LABEL_52', 53: 'LABEL_53', 54: 'LABEL_54', 55: 'LABEL_55', 56: 'LABEL_56', 57: 'LABEL_57', 58: 'LABEL_58', 59: 'LABEL_59', 60: 'LABEL_60', 61: 'LABEL_61', 62: 'LABEL_62', 63: 'LABEL_63', 64: 'LABEL_64', 65: 'LABEL_65', 66: 'LABEL_66', 67: 'LABEL_67', 68: 'LABEL_68', 69: 'LABEL_69', 70: 'LABEL_70', 71: 'LABEL_71', 72: 'LABEL_72', 73: 'LABEL_73', 74: 'LABEL_74', 75: 'LABEL_75', 76: 'LABEL_76', 77: 'LABEL_77', 78: 'LABEL_78', 79: 'LABEL_79', 80: 'LABEL_80', 81: 'LABEL_81', 82: 'LABEL_82', 83: 'LABEL_83', 84: 'LABEL_84', 85: 'LABEL_85', 86: 'LABEL_86', 87: 'LABEL_87', 88: 'LABEL_88', 89: 'LABEL_89', 90: 'LABEL_90', 91: 'LABEL_91', 92: 'LABEL_92', 93: 'LABEL_93', 94: 'LABEL_94', 95: 'LABEL_95', 96: 'LABEL_96', 97: 'LABEL_97', 98: 'LABEL_98', 99: 'LABEL_99', 100: 'LABEL_100', 101: 'LABEL_101', 102: 'LABEL_102', 103: 'LABEL_103', 104: 'LABEL_104', 105: 'LABEL_105', 106: 'LABEL_106', 107: 'LABEL_107', 108: 'LABEL_108', 109: 'LABEL_109', 110: 'LABEL_110', 111: 'LABEL_111', 112: 'LABEL_112', 113: 'LABEL_113', 114: 'LABEL_114', 115: 'LABEL_115', 116: 'LABEL_116', 117: 'LABEL_117', 118: 'LABEL_118', 119: 'LABEL_119', 120: 'LABEL_120', 121: 'LABEL_121', 122: 'LABEL_122', 123: 'LABEL_123', 124: 'LABEL_124', 125: 'LABEL_125', 126: 'LABEL_126', 127: 'LABEL_127', 128: 'LABEL_128', 129: 'LABEL_129', 130: 'LABEL_130', 131: 'LABEL_131', 132: 'LABEL_132', 133: 'LABEL_133', 134: 'LABEL_134', 135: 'LABEL_135', 136: 'LABEL_136', 137: 'LABEL_137', 138: 'LABEL_138', 139: 'LABEL_139', 140: 'LABEL_140', 141: 'LABEL_141', 142: 'LABEL_142', 143: 'LABEL_143', 144: 'LABEL_144', 145: 'LABEL_145', 146: 'LABEL_146', 147: 'LABEL_147', 148: 'LABEL_148', 149: 'LABEL_149', 150: 'LABEL_150', 151: 'LABEL_151', 152: 'LABEL_152', 153: 'LABEL_153', 154: 'LABEL_154', 155: 'LABEL_155', 156: 'LABEL_156', 157: 'LABEL_157', 158: 'LABEL_158', 159: 'LABEL_159', 160: 'LABEL_160', 161: 'LABEL_161', 162: 'LABEL_162', 163: 'LABEL_163', 164: 'LABEL_164', 165: 'LABEL_165', 166: 'LABEL_166', 167: 'LABEL_167', 168: 'LABEL_168', 169: 'LABEL_169', 170: 'LABEL_170', 171: 'LABEL_171', 172: 'LABEL_172', 173: 'LABEL_173', 174: 'LABEL_174', 175: 'LABEL_175', 176: 'LABEL_176', 177: 'LABEL_177', 178: 'LABEL_178', 179: 'LABEL_179', 180: 'LABEL_180', 181: 'LABEL_181', 182: 'LABEL_182', 183: 'LABEL_183', 184: 'LABEL_184', 185: 'LABEL_185', 186: 'LABEL_186', 187: 'LABEL_187', 188: 'LABEL_188', 189: 'LABEL_189', 190: 'LABEL_190', 191: 'LABEL_191', 192: 'LABEL_192', 193: 'LABEL_193', 194: 'LABEL_194', 195: 'LABEL_195', 196: 'LABEL_196', 197: 'LABEL_197', 198: 'LABEL_198', 199: 'LABEL_199', 200: 'LABEL_200', 201: 'LABEL_201', 202: 'LABEL_202', 203: 'LABEL_203', 204: 'LABEL_204', 205: 'LABEL_205', 206: 'LABEL_206', 207: 'LABEL_207', 208: 'LABEL_208', 209: 'LABEL_209', 210: 'LABEL_210', 211: 'LABEL_211', 212: 'LABEL_212', 213: 'LABEL_213', 214: 'LABEL_214', 215: 'LABEL_215', 216: 'LABEL_216', 217: 'LABEL_217', 218: 'LABEL_218', 219: 'LABEL_219', 220: 'LABEL_220', 221: 'LABEL_221', 222: 'LABEL_222', 223: 'LABEL_223', 224: 'LABEL_224', 225: 'LABEL_225', 226: 'LABEL_226', 227: 'LABEL_227', 228: 'LABEL_228', 229: 'LABEL_229', 230: 'LABEL_230', 231: 'LABEL_231', 232: 'LABEL_232', 233: 'LABEL_233', 234: 'LABEL_234', 235: 'LABEL_235', 236: 'LABEL_236', 237: 'LABEL_237', 238: 'LABEL_238', 239: 'LABEL_239', 240: 'LABEL_240', 241: 'LABEL_241', 242: 'LABEL_242', 243: 'LABEL_243', 244: 'LABEL_244', 245: 'LABEL_245', 246: 'LABEL_246', 247: 'LABEL_247', 248: 'LABEL_248', 249: 'LABEL_249', 250: 'LABEL_250', 251: 'LABEL_251', 252: 'LABEL_252', 253: 'LABEL_253', 254: 'LABEL_254', 255: 'LABEL_255', 256: 'LABEL_256', 257: 'LABEL_257', 258: 'LABEL_258', 259: 'LABEL_259', 260: 'LABEL_260', 261: 'LABEL_261', 262: 'LABEL_262', 263: 'LABEL_263', 264: 'LABEL_264', 265: 'LABEL_265', 266: 'LABEL_266', 267: 'LABEL_267', 268: 'LABEL_268', 269: 'LABEL_269', 270: 'LABEL_270', 271: 'LABEL_271', 272: 'LABEL_272', 273: 'LABEL_273', 274: 'LABEL_274', 275: 'LABEL_275', 276: 'LABEL_276', 277: 'LABEL_277', 278: 'LABEL_278', 279: 'LABEL_279', 280: 'LABEL_280', 281: 'LABEL_281', 282: 'LABEL_282', 283: 'LABEL_283', 284: 'LABEL_284', 285: 'LABEL_285', 286: 'LABEL_286', 287: 'LABEL_287', 288: 'LABEL_288', 289: 'LABEL_289', 290: 'LABEL_290', 291: 'LABEL_291', 292: 'LABEL_292', 293: 'LABEL_293', 294: 'LABEL_294', 295: 'LABEL_295', 296: 'LABEL_296', 297: 'LABEL_297', 298: 'LABEL_298', 299: 'LABEL_299', 300: 'LABEL_300', 301: 'LABEL_301', 302: 'LABEL_302', 303: 'LABEL_303', 304: 'LABEL_304', 305: 'LABEL_305', 306: 'LABEL_306', 307: 'LABEL_307', 308: 'LABEL_308', 309: 'LABEL_309', 310: 'LABEL_310', 311: 'LABEL_311', 312: 'LABEL_312', 313: 'LABEL_313', 314: 'LABEL_314', 315: 'LABEL_315', 316: 'LABEL_316', 317: 'LABEL_317', 318: 'LABEL_318', 319: 'LABEL_319', 320: 'LABEL_320', 321: 'LABEL_321', 322: 'LABEL_322', 323: 'LABEL_323', 324: 'LABEL_324', 325: 'LABEL_325', 326: 'LABEL_326', 327: 'LABEL_327', 328: 'LABEL_328', 329: 'LABEL_329', 330: 'LABEL_330', 331: 'LABEL_331', 332: 'LABEL_332', 333: 'LABEL_333', 334: 'LABEL_334', 335: 'LABEL_335', 336: 'LABEL_336', 337: 'LABEL_337', 338: 'LABEL_338', 339: 'LABEL_339', 340: 'LABEL_340', 341: 'LABEL_341', 342: 'LABEL_342', 343: 'LABEL_343', 344: 'LABEL_344', 345: 'LABEL_345', 346: 'LABEL_346', 347: 'LABEL_347', 348: 'LABEL_348', 349: 'LABEL_349', 350: 'LABEL_350', 351: 'LABEL_351', 352: 'LABEL_352', 353: 'LABEL_353', 354: 'LABEL_354', 355: 'LABEL_355', 356: 'LABEL_356', 357: 'LABEL_357', 358: 'LABEL_358', 359: 'LABEL_359', 360: 'LABEL_360', 361: 'LABEL_361', 362: 'LABEL_362', 363: 'LABEL_363', 364: 'LABEL_364', 365: 'LABEL_365', 366: 'LABEL_366', 367: 'LABEL_367', 368: 'LABEL_368', 369: 'LABEL_369', 370: 'LABEL_370', 371: 'LABEL_371', 372: 'LABEL_372', 373: 'LABEL_373', 374: 'LABEL_374', 375: 'LABEL_375', 376: 'LABEL_376', 377: 'LABEL_377', 378: 'LABEL_378', 379: 'LABEL_379', 380: 'LABEL_380', 381: 'LABEL_381', 382: 'LABEL_382', 383: 'LABEL_383', 384: 'LABEL_384', 385: 'LABEL_385', 386: 'LABEL_386', 387: 'LABEL_387', 388: 'LABEL_388', 389: 'LABEL_389'}\" for key \"id2label\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n",
      "Trainer is attempting to log a value of \"{'LABEL_0': 0, 'LABEL_1': 1, 'LABEL_2': 2, 'LABEL_3': 3, 'LABEL_4': 4, 'LABEL_5': 5, 'LABEL_6': 6, 'LABEL_7': 7, 'LABEL_8': 8, 'LABEL_9': 9, 'LABEL_10': 10, 'LABEL_11': 11, 'LABEL_12': 12, 'LABEL_13': 13, 'LABEL_14': 14, 'LABEL_15': 15, 'LABEL_16': 16, 'LABEL_17': 17, 'LABEL_18': 18, 'LABEL_19': 19, 'LABEL_20': 20, 'LABEL_21': 21, 'LABEL_22': 22, 'LABEL_23': 23, 'LABEL_24': 24, 'LABEL_25': 25, 'LABEL_26': 26, 'LABEL_27': 27, 'LABEL_28': 28, 'LABEL_29': 29, 'LABEL_30': 30, 'LABEL_31': 31, 'LABEL_32': 32, 'LABEL_33': 33, 'LABEL_34': 34, 'LABEL_35': 35, 'LABEL_36': 36, 'LABEL_37': 37, 'LABEL_38': 38, 'LABEL_39': 39, 'LABEL_40': 40, 'LABEL_41': 41, 'LABEL_42': 42, 'LABEL_43': 43, 'LABEL_44': 44, 'LABEL_45': 45, 'LABEL_46': 46, 'LABEL_47': 47, 'LABEL_48': 48, 'LABEL_49': 49, 'LABEL_50': 50, 'LABEL_51': 51, 'LABEL_52': 52, 'LABEL_53': 53, 'LABEL_54': 54, 'LABEL_55': 55, 'LABEL_56': 56, 'LABEL_57': 57, 'LABEL_58': 58, 'LABEL_59': 59, 'LABEL_60': 60, 'LABEL_61': 61, 'LABEL_62': 62, 'LABEL_63': 63, 'LABEL_64': 64, 'LABEL_65': 65, 'LABEL_66': 66, 'LABEL_67': 67, 'LABEL_68': 68, 'LABEL_69': 69, 'LABEL_70': 70, 'LABEL_71': 71, 'LABEL_72': 72, 'LABEL_73': 73, 'LABEL_74': 74, 'LABEL_75': 75, 'LABEL_76': 76, 'LABEL_77': 77, 'LABEL_78': 78, 'LABEL_79': 79, 'LABEL_80': 80, 'LABEL_81': 81, 'LABEL_82': 82, 'LABEL_83': 83, 'LABEL_84': 84, 'LABEL_85': 85, 'LABEL_86': 86, 'LABEL_87': 87, 'LABEL_88': 88, 'LABEL_89': 89, 'LABEL_90': 90, 'LABEL_91': 91, 'LABEL_92': 92, 'LABEL_93': 93, 'LABEL_94': 94, 'LABEL_95': 95, 'LABEL_96': 96, 'LABEL_97': 97, 'LABEL_98': 98, 'LABEL_99': 99, 'LABEL_100': 100, 'LABEL_101': 101, 'LABEL_102': 102, 'LABEL_103': 103, 'LABEL_104': 104, 'LABEL_105': 105, 'LABEL_106': 106, 'LABEL_107': 107, 'LABEL_108': 108, 'LABEL_109': 109, 'LABEL_110': 110, 'LABEL_111': 111, 'LABEL_112': 112, 'LABEL_113': 113, 'LABEL_114': 114, 'LABEL_115': 115, 'LABEL_116': 116, 'LABEL_117': 117, 'LABEL_118': 118, 'LABEL_119': 119, 'LABEL_120': 120, 'LABEL_121': 121, 'LABEL_122': 122, 'LABEL_123': 123, 'LABEL_124': 124, 'LABEL_125': 125, 'LABEL_126': 126, 'LABEL_127': 127, 'LABEL_128': 128, 'LABEL_129': 129, 'LABEL_130': 130, 'LABEL_131': 131, 'LABEL_132': 132, 'LABEL_133': 133, 'LABEL_134': 134, 'LABEL_135': 135, 'LABEL_136': 136, 'LABEL_137': 137, 'LABEL_138': 138, 'LABEL_139': 139, 'LABEL_140': 140, 'LABEL_141': 141, 'LABEL_142': 142, 'LABEL_143': 143, 'LABEL_144': 144, 'LABEL_145': 145, 'LABEL_146': 146, 'LABEL_147': 147, 'LABEL_148': 148, 'LABEL_149': 149, 'LABEL_150': 150, 'LABEL_151': 151, 'LABEL_152': 152, 'LABEL_153': 153, 'LABEL_154': 154, 'LABEL_155': 155, 'LABEL_156': 156, 'LABEL_157': 157, 'LABEL_158': 158, 'LABEL_159': 159, 'LABEL_160': 160, 'LABEL_161': 161, 'LABEL_162': 162, 'LABEL_163': 163, 'LABEL_164': 164, 'LABEL_165': 165, 'LABEL_166': 166, 'LABEL_167': 167, 'LABEL_168': 168, 'LABEL_169': 169, 'LABEL_170': 170, 'LABEL_171': 171, 'LABEL_172': 172, 'LABEL_173': 173, 'LABEL_174': 174, 'LABEL_175': 175, 'LABEL_176': 176, 'LABEL_177': 177, 'LABEL_178': 178, 'LABEL_179': 179, 'LABEL_180': 180, 'LABEL_181': 181, 'LABEL_182': 182, 'LABEL_183': 183, 'LABEL_184': 184, 'LABEL_185': 185, 'LABEL_186': 186, 'LABEL_187': 187, 'LABEL_188': 188, 'LABEL_189': 189, 'LABEL_190': 190, 'LABEL_191': 191, 'LABEL_192': 192, 'LABEL_193': 193, 'LABEL_194': 194, 'LABEL_195': 195, 'LABEL_196': 196, 'LABEL_197': 197, 'LABEL_198': 198, 'LABEL_199': 199, 'LABEL_200': 200, 'LABEL_201': 201, 'LABEL_202': 202, 'LABEL_203': 203, 'LABEL_204': 204, 'LABEL_205': 205, 'LABEL_206': 206, 'LABEL_207': 207, 'LABEL_208': 208, 'LABEL_209': 209, 'LABEL_210': 210, 'LABEL_211': 211, 'LABEL_212': 212, 'LABEL_213': 213, 'LABEL_214': 214, 'LABEL_215': 215, 'LABEL_216': 216, 'LABEL_217': 217, 'LABEL_218': 218, 'LABEL_219': 219, 'LABEL_220': 220, 'LABEL_221': 221, 'LABEL_222': 222, 'LABEL_223': 223, 'LABEL_224': 224, 'LABEL_225': 225, 'LABEL_226': 226, 'LABEL_227': 227, 'LABEL_228': 228, 'LABEL_229': 229, 'LABEL_230': 230, 'LABEL_231': 231, 'LABEL_232': 232, 'LABEL_233': 233, 'LABEL_234': 234, 'LABEL_235': 235, 'LABEL_236': 236, 'LABEL_237': 237, 'LABEL_238': 238, 'LABEL_239': 239, 'LABEL_240': 240, 'LABEL_241': 241, 'LABEL_242': 242, 'LABEL_243': 243, 'LABEL_244': 244, 'LABEL_245': 245, 'LABEL_246': 246, 'LABEL_247': 247, 'LABEL_248': 248, 'LABEL_249': 249, 'LABEL_250': 250, 'LABEL_251': 251, 'LABEL_252': 252, 'LABEL_253': 253, 'LABEL_254': 254, 'LABEL_255': 255, 'LABEL_256': 256, 'LABEL_257': 257, 'LABEL_258': 258, 'LABEL_259': 259, 'LABEL_260': 260, 'LABEL_261': 261, 'LABEL_262': 262, 'LABEL_263': 263, 'LABEL_264': 264, 'LABEL_265': 265, 'LABEL_266': 266, 'LABEL_267': 267, 'LABEL_268': 268, 'LABEL_269': 269, 'LABEL_270': 270, 'LABEL_271': 271, 'LABEL_272': 272, 'LABEL_273': 273, 'LABEL_274': 274, 'LABEL_275': 275, 'LABEL_276': 276, 'LABEL_277': 277, 'LABEL_278': 278, 'LABEL_279': 279, 'LABEL_280': 280, 'LABEL_281': 281, 'LABEL_282': 282, 'LABEL_283': 283, 'LABEL_284': 284, 'LABEL_285': 285, 'LABEL_286': 286, 'LABEL_287': 287, 'LABEL_288': 288, 'LABEL_289': 289, 'LABEL_290': 290, 'LABEL_291': 291, 'LABEL_292': 292, 'LABEL_293': 293, 'LABEL_294': 294, 'LABEL_295': 295, 'LABEL_296': 296, 'LABEL_297': 297, 'LABEL_298': 298, 'LABEL_299': 299, 'LABEL_300': 300, 'LABEL_301': 301, 'LABEL_302': 302, 'LABEL_303': 303, 'LABEL_304': 304, 'LABEL_305': 305, 'LABEL_306': 306, 'LABEL_307': 307, 'LABEL_308': 308, 'LABEL_309': 309, 'LABEL_310': 310, 'LABEL_311': 311, 'LABEL_312': 312, 'LABEL_313': 313, 'LABEL_314': 314, 'LABEL_315': 315, 'LABEL_316': 316, 'LABEL_317': 317, 'LABEL_318': 318, 'LABEL_319': 319, 'LABEL_320': 320, 'LABEL_321': 321, 'LABEL_322': 322, 'LABEL_323': 323, 'LABEL_324': 324, 'LABEL_325': 325, 'LABEL_326': 326, 'LABEL_327': 327, 'LABEL_328': 328, 'LABEL_329': 329, 'LABEL_330': 330, 'LABEL_331': 331, 'LABEL_332': 332, 'LABEL_333': 333, 'LABEL_334': 334, 'LABEL_335': 335, 'LABEL_336': 336, 'LABEL_337': 337, 'LABEL_338': 338, 'LABEL_339': 339, 'LABEL_340': 340, 'LABEL_341': 341, 'LABEL_342': 342, 'LABEL_343': 343, 'LABEL_344': 344, 'LABEL_345': 345, 'LABEL_346': 346, 'LABEL_347': 347, 'LABEL_348': 348, 'LABEL_349': 349, 'LABEL_350': 350, 'LABEL_351': 351, 'LABEL_352': 352, 'LABEL_353': 353, 'LABEL_354': 354, 'LABEL_355': 355, 'LABEL_356': 356, 'LABEL_357': 357, 'LABEL_358': 358, 'LABEL_359': 359, 'LABEL_360': 360, 'LABEL_361': 361, 'LABEL_362': 362, 'LABEL_363': 363, 'LABEL_364': 364, 'LABEL_365': 365, 'LABEL_366': 366, 'LABEL_367': 367, 'LABEL_368': 368, 'LABEL_369': 369, 'LABEL_370': 370, 'LABEL_371': 371, 'LABEL_372': 372, 'LABEL_373': 373, 'LABEL_374': 374, 'LABEL_375': 375, 'LABEL_376': 376, 'LABEL_377': 377, 'LABEL_378': 378, 'LABEL_379': 379, 'LABEL_380': 380, 'LABEL_381': 381, 'LABEL_382': 382, 'LABEL_383': 383, 'LABEL_384': 384, 'LABEL_385': 385, 'LABEL_386': 386, 'LABEL_387': 387, 'LABEL_388': 388, 'LABEL_389': 389}\" for key \"label2id\" as a parameter. MLflow's log_param() only accepts values no longer than 250 characters so we dropped this attribute. You can use `MLFLOW_FLATTEN_PARAMS` environment variable to flatten the parameters and avoid this message.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='156' max='2700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 156/2700 16:34 < 4:33:55, 0.15 it/s, Epoch 2.54/45]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.451004</td>\n",
       "      <td>0.202265</td>\n",
       "      <td>0.159461</td>\n",
       "      <td>0.212524</td>\n",
       "      <td>0.202265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.686080</td>\n",
       "      <td>0.386773</td>\n",
       "      <td>0.334021</td>\n",
       "      <td>0.387802</td>\n",
       "      <td>0.386773</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\noemi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\noemi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "# Define metrics for evaluation\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./best_model\",  # Save only the best model\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",  # Align save strategy with evaluation strategy\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=128,\n",
    "    gradient_accumulation_steps=4,  # Reduced to fit memory constraints\n",
    "    per_device_eval_batch_size=128,\n",
    "    num_train_epochs=45,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=1,  # Save only the best checkpoint\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(tokenized_datasets['test'])\n",
    "print(\"Test Results:\", test_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

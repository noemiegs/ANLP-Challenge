{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, EarlyStoppingCallback\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "\n",
    "data = pd.read_csv('data/train_submission.csv')\n",
    "print(len(data.Label.unique()))\n",
    "data = data.dropna()\n",
    "print(data.shape)\n",
    "data = data.drop(columns=['Usage'])\n",
    "data_augmented = pd.read_csv('data/aug_data.csv')\n",
    "data = pd.concat([data, data_augmented], ignore_index=True)\n",
    "data['numWord'] = data['Text'].apply(lambda x: len(str(x).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls():\n",
    "    url_pattern = r'https://\\S+'\n",
    "    counter = 0\n",
    "# Loop through the 'Text' column to remove URLs\n",
    "    for index, row in data.iterrows():\n",
    "        original_text = row['Text']\n",
    "    \n",
    "    # Find all URLs in the text\n",
    "        urls = re.findall(url_pattern, original_text)\n",
    "    \n",
    "    # Print and remove each URL from the text\n",
    "        if urls:\n",
    "            for url in urls:\n",
    "                # print(f\"Removing URL: {url}\")\n",
    "                counter += 1\n",
    "        \n",
    "        # Remove URLs from the text\n",
    "            cleaned_text = re.sub(url_pattern, '', original_text)\n",
    "        \n",
    "        # Update the DataFrame with the cleaned text\n",
    "            data.at[index, 'Text'] = cleaned_text\n",
    "    print(f\"Removed {counter} URLs from the 'Text' column.\")\n",
    "\n",
    "REM_URL = False\n",
    "if REM_URL:\n",
    "    remove_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# set random seed for reproducibility\n",
    "random.seed(1)\n",
    "\n",
    "label_counts = data.Label.value_counts()\n",
    "data_aug = pd.DataFrame(columns=['Text', 'Label'])\n",
    "\n",
    "for label, count in label_counts[label_counts < 10].items():\n",
    "    label_data = data[data['Label'] == label]\n",
    "    current_count = len(label_data)\n",
    "\n",
    "    idx = 0 \n",
    "    while current_count < 10:\n",
    "        # Get the current example in sequential order\n",
    "        example = label_data.iloc[idx]['Text']\n",
    "        \n",
    "        # Split the example into words\n",
    "        words = example.split()\n",
    "        \n",
    "        # Shuffle the words\n",
    "        random.shuffle(words)\n",
    "        \n",
    "        # Reconstruct the sentence from shuffled words\n",
    "        new_example = ' '.join(words)\n",
    "        \n",
    "        # Create a new DataFrame for the new example\n",
    "        new_data = pd.DataFrame({'Text': [new_example], 'Label': [label]})\n",
    "        \n",
    "        # Concatenate the new data to the original DataFrame\n",
    "        data_aug = pd.concat([data_aug, new_data], ignore_index=True)\n",
    "        current_count += 1\n",
    "        \n",
    "        # Move to the next example in the list (sequential)\n",
    "        idx += 1\n",
    "        if idx >= len(label_data):  # If we've processed all examples, restart from the beginning\n",
    "            idx = 0\n",
    "print(data_aug)\n",
    "\n",
    "data = pd.concat([data, data_aug], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_counts = data['Label'].value_counts()\n",
    "\n",
    "REMOVE_OCCURENCES = True\n",
    "\n",
    "if REMOVE_OCCURENCES:\n",
    "    for label, count in label_counts.items():\n",
    "        if count > 500:\n",
    "            # Get the rows for the current label\n",
    "            label_data = data[data['Label'] == label]\n",
    "            \n",
    "            # Number of rows to delete to make the count 500\n",
    "            rows_to_delete = count - 500\n",
    "            # print(f\"Label: {label} | Rows to delete: {rows_to_delete}\")\n",
    "            \n",
    "            # Randomly shuffle the rows of this label\n",
    "\n",
    "            rows_to_delete_indices = random.sample(label_data.index.tolist(), rows_to_delete)\n",
    "            \n",
    "            # Delete the rows from the DataFrame\n",
    "            data = data.drop(rows_to_delete_indices)\n",
    "            \n",
    "            # Print how many rows were deleted for this label\n",
    "            print(f\"Deleted {rows_to_delete} rows for label '{label}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH_NAME = \"test_intfloat\" # TO CHANGE FOR EACH MODEL AT CONVENIENCE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "labels = data['Label'].unique()\n",
    "label2id = {label: idx for idx, label in enumerate(labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "data['LabelID'] = data['Label'].map(label2id)\n",
    "\n",
    "mappings = {\"label2id\": label2id, \"id2label\": id2label}\n",
    "\n",
    "path_mapping = MODEL_PATH_NAME + \"_mappings.json\"\n",
    "with open(path_mapping, \"w\") as f:\n",
    "    json.dump(mappings, f, indent=4)\n",
    "\n",
    "import shutil\n",
    "shutil.move(path_mapping, \"data/mapping/\" + path_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[\"Text\"]\n",
    "y = data[\"LabelID\"]\n",
    "\n",
    "train_texts, test_texts, train_labels, test_labels = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(test_texts, test_labels, test_size=0.5, random_state=42, stratify=test_labels)\n",
    "\n",
    "train_dataset = Dataset.from_pandas(pd.DataFrame({'Text': train_texts, 'LabelID': train_labels}))\n",
    "val_dataset = Dataset.from_pandas(pd.DataFrame({'Text': val_texts, 'LabelID': val_labels}))\n",
    "test_dataset = Dataset.from_pandas(pd.DataFrame({'Text': test_texts, 'LabelID': test_labels}))\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'validation': val_dataset,\n",
    "    'test': test_dataset\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"intfloat/multilingual-e5-large-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = tokenizer(examples['Text'], truncation=True, padding=True, max_length=100)\n",
    "    inputs[\"labels\"] = examples[\"LabelID\"]\n",
    "    return inputs\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define metrics for evaluation\n",
    "def compute_metrics(pred):\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    labels = pred.label_ids\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./\" + MODEL_PATH_NAME,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=32,  \n",
    "    gradient_accumulation_steps=2,  \n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=5,  \n",
    "    # weight_decay=0.01,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,\n",
    "    greater_is_better=True\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['validation'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)] \n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate on the test set\n",
    "test_results = trainer.evaluate(tokenized_datasets['test'])\n",
    "print(\"Test Results:\", test_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
